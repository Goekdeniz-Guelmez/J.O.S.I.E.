Based on the paper, I'll break down the architecture of Mimi, which is designed as a neural audio codec that combines both semantic and acoustic tokenization. Here are the key components:

1. Base Architecture:
- Uses a SeaNet autoencoder structure with both encoder and decoder
- Input: Single-channel 24kHz waveform
- Output: Latent representation at 12.5 frames per second with dimension D=512
- All convolutions are causal to enable streaming

2. Encoder:
- Contains 4 convolutional blocks with:
  - Dilated convolutions
  - Strided convolutions
  - ELU non-linearities
  - Weight Normalization
- Striding factors: (4, 5, 6, 8)
- Final 1D convolution with stride 2

- Includes a Transformer in the bottleneck (pre-quantization):
  - 8 layers
  - 8 heads
  - RoPE position encodings
  - 250 frame context (~20 seconds)
  - Model dimension 512
  - MLP dimension 2048
  - GELU activations
  - LayerScale initialization at 0.01

3. Decoder:
- Mirror structure of encoder but with transposed convolutions
- Also includes a Transformer after quantization with same specs as encoder Transformer
- Projects back to waveform domain

4. Quantization:
- Uses Split Residual Vector Quantization (RVQ)
- Two parallel paths:
  a. Semantic path: Single Vector Quantizer for semantic tokens
  b. Acoustic path: RVQ with 7 levels for acoustic tokens
- Total of 8 codebooks each with 2048 entries
- Operates at 12.5Hz frame rate
- Achieves 1.1kbps bitrate
- Projects embeddings to 256 dimensions before RVQ and back to 512 after

5. Training Features:
- Uses AdamW optimizer
- Weight decay of 5e-2 (only on Transformer parameters)
- Learning rate of 8e-4
- Batch size of 128 with 12s windows
- Uses quantizer dropout
- Applies quantization only 50% of the time during training
- Trained with adversarial losses only (no reconstruction losses)

6. Semantic Distillation:
- Distills knowledge from WavLM into the first quantizer
- Downsamples input to 16kHz for WavLM processing
- Uses linear projection to 1024 dimensions
- Applies non-causal average pooling (stride 4, kernel size 8)
- Computes cosine distance for distillation loss

The key innovation is the split RVQ design that separates semantic and acoustic tokens while maintaining high audio quality and streaming capability. This allows Mimi to encode both linguistic content and audio details in a format suitable for real-time generation.
